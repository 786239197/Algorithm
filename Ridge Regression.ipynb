{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本笔记由马景义, 赖楸鸿完成, 如有任何错误欢迎向两位编者反馈."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一些常用的公式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（1）链式法则：若$y(x)$为$x$的向量函数，则\n",
    "$$ \\frac{\\partial f(y(x))}{\\partial x} = \\frac{\\partial y^T(x)}{\\partial x}\\frac{\\partial f(y)}{\\partial y} ,$$\n",
    "其中$\\frac{\\partial y^T(x)}{\\partial x}$为$n\\times n$矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（2）$a$为$n\\times1$常数向量，则\n",
    "$$\\frac{\\partial a^Tx}{\\partial x}= a ,$$\n",
    "$$\\frac{\\partial x^Ta}{\\partial x}= a.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（3）$a$为$n\\times1$常数向量，则\n",
    "$$\\frac{\\partial a^Ty(x)}{\\partial x}=\\frac{\\partial y^T(x)}{\\partial x}a ,$$\n",
    "$$\\frac{\\partial y^T(x)a}{\\partial x}=\\frac{\\partial y^T(x)}{\\partial x}a.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（4）矩阵$A$和向量$y$与$x$无关，则$$\\frac{\\partial x^TAy}{\\partial x}=Ay ,$$\n",
    "$$\\frac{\\partial y^TAx}{\\partial x}= A^Ty.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（5）矩阵$A$和向量$y$与$x$无关，则\n",
    "$$\\frac{\\partial x^TA}{\\partial x}=A ,$$\n",
    "$$\\frac{\\partial x^TAx}{\\partial x}=Ax+A^Tx=(A+A^T)x.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "链式法则搭配后面四条可以衍生出许多公式，感觉已经非常够用了~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 岭回归的求导部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "L(\\beta)=(Y-X\\beta)^T(Y-X\\beta)+\\beta^T\\lambda\\beta ,$$\n",
    "其中$Y$为$n\\times1$向量，$X$为$n\\times p+1$矩阵，$\\beta$为$p+1\\times1$向量，$\\lambda$为$p+1\\times p+1$矩阵,即\n",
    "$$\\lambda= \\left[\\begin{matrix}0 &0&\\cdots&0 \\\\ 0&\\lambda&\\cdots&0\\\\ \\vdots&\\vdots&\\ddots&\\vdots\\\\0&0&\\cdots&\\lambda \\end{matrix}\\right].$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "&\\frac{\\partial L(\\beta)}{\\partial \\beta} \\\\\n",
    "=&\\frac{\\partial (Y-X\\beta)^T(Y-X\\beta)}{\\partial \\beta}+\\frac{\\partial \\beta^T\\lambda\\beta}{\\partial \\beta} \\\\\n",
    "=&\\frac{\\partial (Y-X\\beta)^T}{\\partial \\beta}[(Y-X\\beta)+(Y-X\\beta)]+(\\lambda+\\lambda^T)\\beta \\\\\n",
    "=&-2X^T(Y-X\\beta)+2\\lambda\\beta\n",
    "\\end{split}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###mchol函数将对称方阵分解为一个下三角矩阵乘以该矩阵转置的形式,函数返回值为下三角矩阵\n",
    "#输入：欲分解的矩阵x\n",
    "#输出：cholesky分解所得矩阵L\n",
    "#cholesky decomposition详见：https://en.wikipedia.org/wiki/Cholesky_decomposition\n",
    "mchol <- function(x)\n",
    "{\n",
    "  #求矩阵x的行列数,m为行数,n为列数\n",
    "  mn <- dim(x)\n",
    "  m <- mn[1]\n",
    "  n <- mn[2]\n",
    "\n",
    "  #检验x是否为方阵\n",
    "  if(m != n) \n",
    "  {\n",
    "    return (\"Wrong dimensions of matrix!\")\n",
    "  }\n",
    "\n",
    "  #检验x是否为对称矩阵\n",
    "  if(sum(t(x) != x) > 0) \n",
    "  {\n",
    "    return (\"Input matrix is not symmetrical!\")\n",
    "  }\n",
    "\n",
    "  #L为与x行列数相等的零矩阵，用于存放分解所得下三角矩阵\n",
    "  L <- matrix(0, m, m)\n",
    "  \n",
    "  #循环每进行一次,求解一列矩阵L的元素\n",
    "  #矩阵x第i列和第i行之前的元素不再使用，相当于矩阵x减少一个维数，故下述将循环所至第i列记为当前矩阵x和矩阵L的第一列\n",
    "  for(i in 1:m)\n",
    "  {\n",
    "    #L的主对角线上第i个元素为x的主对角线上第i个元素开方\n",
    "    L[i,i] <- sqrt(x[i,i])\n",
    "    if(i < m)\n",
    "    {\n",
    "      #求当前矩阵(L的子矩阵）的第一列除第一个元素外的其他元素\n",
    "      L[(i+1):m,i] <- x[(i+1):m,i]/L[i,i]\n",
    "\n",
    "      #矩阵L第一列（除第一个元素）乘以它的转置得到TLM用于更新矩阵x，效果同TLM%*%TLM\n",
    "      TLV <- L[(i+1):m,i]#记录已求出第一列除第一个元素外剩下元素\n",
    "      TLM <- matrix(TLV, m-i, m-i)#TLV按列复制成矩阵\n",
    "      TLM <- sweep(TLM, 2, TLV, \"*\")#sweep(x， MARGIN， STATS， FUN=”-“， …) 对矩阵进行运算\n",
    "                                    #MARGIN为1，表示行的方向上进行运算，为2表示列的方向上运算(是指将参数从列的方向移下去算)\n",
    "                                    #STATS是运算的参数，FUN为运算函数，默认是减法\n",
    "\n",
    "      #减少一个维数的矩阵x更新为原来对应位置上的元素减去TLM，为下一次循环做准备\n",
    "      x[(i+1):m,(i+1):m] <- x[(i+1):m,(i+1):m] - TLM\n",
    "    }\n",
    "  }\n",
    "  #矩阵的返回值为我们要求的下三角矩阵L\n",
    "  L  \n",
    "}\n",
    "\n",
    "######EXAM\n",
    "#y=matrix(rnorm(20),5)\n",
    "#x=t(y)%*%y\n",
    "#mchol(x)\n",
    "######EXAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###mforwardsolve函数求解线性方程租Lx=b，其中L为下三角矩阵\n",
    "#输入：下三角矩阵L，向量b\n",
    "#输出：线性方程组的解x\n",
    "mforwardsolve <- function(L, b)\n",
    "{\n",
    "  #求L的行列数,m为L的行数,n为L的列数\n",
    "  mn <- dim(L)\n",
    "  m <- mn[1]\n",
    "  n <- mn[2]\n",
    "\n",
    "  #判断L是否为方阵\n",
    "  if(m != n) \n",
    "  {\n",
    "    return (\"Wrong dimensions of matrix L!\")\n",
    "  }\n",
    "  \n",
    "  #判断L是否为下三角矩阵\n",
    "  for (i in 1:(m-1))\n",
    "  {\n",
    "    if(sum(L[i,(i+1):m] != 0) > 0)#逐行判断上三角是否全为0元素\n",
    "    {\n",
    "      return (\"Matrix L must be a lower triangular matrix!\")\n",
    "    }\n",
    "  }\n",
    "\n",
    "  #判断L的行数与b的长度是否相等\n",
    "  if(m != length(b))\n",
    "  {\n",
    "    return (\"Wrong dimensions of matrix L or vector b!\")\n",
    "  }\n",
    "  \n",
    "  #0向量记录求解结果\n",
    "  x=rep(0, m)\n",
    "  \n",
    "  #循环每进行一次,求解一个x中的元素，看作矩阵L向量x向量b的维数减一\n",
    "  #故下述将矩阵L的第i列记为当前矩阵L第一列，将向量x向量b的第i个元素记为当前向量第一个元素\n",
    "  for(i in 1:m)\n",
    "  {\n",
    "    #求当前循环中x的第一个元素\n",
    "    x[i] <- b[i] / L[i,i]\n",
    "    #降维后的b向量为原来位置上的元素减去当前矩阵L的第一列的乘积\n",
    "    if(i < m) \n",
    "    {\n",
    "      b[(i+1):m] <- b[(i+1):m] - x[i]*L[(i+1):m,i]\n",
    "    }      \n",
    "  }\n",
    "  #函数返回的x向量即为线性方程组的解\n",
    "  x  \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "###mbacksolve函数求解线性方程租Lx=b，其中L为上三角矩阵\n",
    "#输入：上三角矩阵L，向量b\n",
    "#输出：线性方程组的解x\n",
    "mbacksolve <- function(L, b)\n",
    "{\n",
    "  #求L的行列数,m为L的行数,n为L的列数\n",
    "  mn <-dim(L)\n",
    "  m <- mn[1]\n",
    "  n <- mn[2]\n",
    "\n",
    "  #判断L是否为方阵\n",
    "  if(m != n)\n",
    "  {  \n",
    "    return (\"Wrong dimensions of matrix L!\")\n",
    "  }\n",
    "\n",
    "  #判断L是否为上三角矩阵\n",
    "  for (i in 2:m)\n",
    "  {\n",
    "    if(sum(L[i,1:(i-1)] != 0) > 0)\n",
    "    {\n",
    "      return (\"Matrix L must be a upper triangular matrix!\")\n",
    "    }\n",
    "  }\n",
    "\n",
    "  #判断L的行数与b的列数是否相等\n",
    "  if(m != length(b))\n",
    "  {\n",
    "    return (\"Wrong dimensions of matrix L or vector b!\")\n",
    "  }  \n",
    "  \n",
    "  x <- rep(0, m)\n",
    "  \n",
    "  #循环每进行一次,求解一个x中的元素，看作矩阵L向量x向量b的维数减一\n",
    "  #故下述将矩阵L的第i列记为当前矩阵L最后一列，将向量x向量b的第i个元素记为当前向量最后一个元素\n",
    "  for(i in m:1)\n",
    "  {\n",
    "    #求当前循环中x的最后一个元素\n",
    "    x[i] <- b[i] / L[i,i]\n",
    "    #降维后的向量b为原来位置上的元素减去刚才求出的x元素与当前上三角矩阵L最后一列（除最后一个元素）的乘积\n",
    "    if(i > 1) \n",
    "    {\n",
    "      b[(i-1):1] <- b[(i-1):1] - x[i]*L[(i-1):1,i]\n",
    "    }      \n",
    "  }\n",
    "  #函数返回值x向量即为线性方程组的解\n",
    "  x  \n",
    "}\n",
    "\n",
    "\n",
    "######EXAM\n",
    "#y=matrix(rnorm(20),5)\n",
    "#x=t(y)%*%y\n",
    "#L=mchol(x); b=1:4\n",
    "#mforwardsolve(L,b)\n",
    "#forwardsolve(L,b)\n",
    "######EXAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###ridgereg函数用于实现岭回归参数beta的估计，参数x和y分别为回归方程的自变量和因变量,lambda为L2正则项的超参数（调整模型的光滑程度，模型随着lambda取值增大光滑度下降）\n",
    "#此函数求解线性方程租(t(x)%*%x+lambada)%*%beta=t(x)%*%y,将t(x)%*%x+lambada进行cholesky分解为L%*%t(L),forwardsolve求解L%*%d=t(x)%*%y,其中d=t(L)%*%beta,backsolve求解t(L)%*%beta=d,即得参数beta的估计值 \n",
    "#岭回归详见：https://baike.baidu.com/item/%E5%B2%AD%E5%9B%9E%E5%BD%92/554917?fr=aladdin\n",
    "#Tikhonov regularization详见：https://en.wikipedia.org/wiki/Tikhonov_regularization\n",
    "#输入：自变量x，因变量y，超参数lambda\n",
    "#输出：回归系数beta的估计值\n",
    "ridgereg <- function(lambda, x, y)\n",
    "{\n",
    "  #y=data[,m]; x=data[,-m]\n",
    "  #n为自变量矩阵行数,即n个样本,p为自变量矩阵列数,即p个参数\n",
    "  np <- dim(x)\n",
    "  n <- np[1]\n",
    "  p <- np[2]\n",
    "\n",
    "  #将自变量矩阵增加一列全1元素,以便于截距项的计算\n",
    "  x <- as.matrix(cbind(rep(1, n),x))\n",
    "\n",
    "  #利用cholesky分解求取回归方程的参数beta的估计值\n",
    "  V <- t(x)%*%x + diag(c(0, rep(lambda, p)))\n",
    "  U <- as.vector(t(x)%*%y)\n",
    "  R <- mchol(V)\n",
    "  M <- mforwardsolve(R, U)\n",
    "  mbacksolve(t(R), M)   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "\t<tr><td>4 </td><td> 6</td><td> 8</td></tr>\n",
       "\t<tr><td>6 </td><td> 9</td><td>12</td></tr>\n",
       "\t<tr><td>8 </td><td>12</td><td>16</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{lll}\n",
       "\t 4  &  6 &  8\\\\\n",
       "\t 6  &  9 & 12\\\\\n",
       "\t 8  & 12 & 16\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| 4  |  6 |  8 | \n",
       "| 6  |  9 | 12 | \n",
       "| 8  | 12 | 16 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "     [,1] [,2] [,3]\n",
       "[1,] 4     6    8  \n",
       "[2,] 6     9   12  \n",
       "[3,] 8    12   16  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outer(2:4,2:4)##这里算是乱入么？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>0.439533558041209</li>\n",
       "\t<li>0.562800409300066</li>\n",
       "\t<li>0.561761682327131</li>\n",
       "\t<li>-0.0198710012557136</li>\n",
       "\t<li>0.0989502953509072</li>\n",
       "\t<li>0.645197525205287</li>\n",
       "\t<li>-0.0777548597849345</li>\n",
       "\t<li>0.0344279766734407</li>\n",
       "\t<li>0.00465563598611401</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 0.439533558041209\n",
       "\\item 0.562800409300066\n",
       "\\item 0.561761682327131\n",
       "\\item -0.0198710012557136\n",
       "\\item 0.0989502953509072\n",
       "\\item 0.645197525205287\n",
       "\\item -0.0777548597849345\n",
       "\\item 0.0344279766734407\n",
       "\\item 0.00465563598611401\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 0.439533558041209\n",
       "2. 0.562800409300066\n",
       "3. 0.561761682327131\n",
       "4. -0.0198710012557136\n",
       "5. 0.0989502953509072\n",
       "6. 0.645197525205287\n",
       "7. -0.0777548597849345\n",
       "8. 0.0344279766734407\n",
       "9. 0.00465563598611401\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1]  0.439533558  0.562800409  0.561761682 -0.019871001  0.098950295\n",
       "[6]  0.645197525 -0.077754860  0.034427977  0.004655636"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'mda' was built under R version 3.4.4\"Loading required package: class\n",
      "Loaded mda 0.4-10\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>lpsa</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td> 0.562800409</td></tr>\n",
       "\t<tr><td> 0.561761682</td></tr>\n",
       "\t<tr><td>-0.019871001</td></tr>\n",
       "\t<tr><td> 0.098950295</td></tr>\n",
       "\t<tr><td> 0.645197525</td></tr>\n",
       "\t<tr><td>-0.077754860</td></tr>\n",
       "\t<tr><td> 0.034427977</td></tr>\n",
       "\t<tr><td> 0.004655636</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{l}\n",
       " lpsa\\\\\n",
       "\\hline\n",
       "\t  0.562800409\\\\\n",
       "\t  0.561761682\\\\\n",
       "\t -0.019871001\\\\\n",
       "\t  0.098950295\\\\\n",
       "\t  0.645197525\\\\\n",
       "\t -0.077754860\\\\\n",
       "\t  0.034427977\\\\\n",
       "\t  0.004655636\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "lpsa | \n",
       "|---|---|---|---|---|---|---|---|\n",
       "|  0.562800409 | \n",
       "|  0.561761682 | \n",
       "| -0.019871001 | \n",
       "|  0.098950295 | \n",
       "|  0.645197525 | \n",
       "| -0.077754860 | \n",
       "|  0.034427977 | \n",
       "|  0.004655636 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "     lpsa        \n",
       "[1,]  0.562800409\n",
       "[2,]  0.561761682\n",
       "[3,] -0.019871001\n",
       "[4,]  0.098950295\n",
       "[5,]  0.645197525\n",
       "[6,] -0.077754860\n",
       "[7,]  0.034427977\n",
       "[8,]  0.004655636"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nAYQEAYQEAYQEAYQEAYQEAYQEAf4DFBAqD3dY0oMAAAAASUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###pred函数的参数b为参数向量,x为自变量,返回值为因变量的预测值\n",
    "#输入：回归系数b向量,数据nx\n",
    "#输出：因变量y的预测值\n",
    "pred <- function(b, nx)\n",
    "{\n",
    "  #nx=prostate[1:2,1:8]\n",
    "  b <- as.vector(b)\n",
    "  p <- length(b) - 1\n",
    "  \n",
    "  #将数据矩阵nx重新排列，每一行为一个样品，重排矩阵的原因是下面例子中调用的数据原结构为dataframe\n",
    "  nx <- as.matrix(nx, ncol <- p)\n",
    "  n <- dim(nx)[1]\n",
    "  \n",
    "  #计算预测值\n",
    "  apply(t(nx)*b[2:(p+1)], 2, sum) + b[1]  \n",
    "}\n",
    "\n",
    "###mridge函数用于实现删去一个样品的岭回归\n",
    "mridge=function(i,lambda,x,y) \n",
    "{\n",
    "    ridgereg(lambda,x[-i,],y[-i])\n",
    "}\n",
    "\n",
    "###cvridgeregerr函数用交叉验证实现岭回归，参数依次为超参数lambda,自变量x(数据矩阵),因变量y,返回值为测试均方误差\n",
    "#交叉验证详见：https://en.wikipedia.org/wiki/Cross-validation_(statistics)    \n",
    "#输入：超参数lambda,自变量x(数据矩阵),因变量y\n",
    "#输出：交叉验证岭回归测试均方误差\n",
    "cvridgeregerr<-function(lambda,x,y){  \n",
    "    \n",
    "    np<-dim(x)\n",
    "    n<-np[1]\n",
    "    p<-np[2]\n",
    "    \n",
    "      \n",
    "    x <- cbind(1,x)\n",
    "    xy <- cbind(x,y)\n",
    "    \n",
    "    #生成总的自变量和因变量矩阵\n",
    "    xtx=t(x)%*%x + diag(c(0, rep(lambda, p)))\n",
    "    xty=t(x)%*%y\n",
    "    \n",
    "    #mrige函数每次删掉一组样本进行回归,最后得到n个模型的参数\n",
    "    mridge <- function(ixy, lambda, xtx, xty){\n",
    "        #生成删掉一组样本后所对应的自变量及因变量\n",
    "        mp=length(ixy)\n",
    "        ix <- ixy[-mp] \n",
    "        iy <- ixy[mp]\n",
    "        \n",
    "        #若利用删除样本的矩阵进行n次计算，计算量过于庞大，因此运用总矩阵先相乘再减去相对应部分，只进行一次矩阵相乘运算以减小运算量\n",
    "        ixtx <- xtx - outer(ix,ix)\n",
    "        ixty <- xty - iy*ix\n",
    "        R <- mchol(ixtx)\n",
    "        M <- mforwardsolve(R, ixty)\n",
    "        mbacksolve(t(R), M)\n",
    "    }\n",
    "    \n",
    "    #矩阵中的元素作为第一个参数输入mridge，表示去掉的数据编号，结果第i行为删去第i个样本的岭回归系数估计值\n",
    "    coe<-t(apply(as.matrix(1:n,ncol=1),1,mridge,lambda,x,y))\n",
    "    #coe第i行和数据矩阵第i个样本做点对点相乘，对行求和，计算测试均方误差\n",
    "    mean((apply(coe*cbind(1,x),1,sum)-y)^2)\n",
    "}\n",
    "\n",
    "\n",
    "     \n",
    "    np <- dim(x)\n",
    "    n <- np[1]\n",
    "    p <- np[2]\n",
    "  \n",
    "\n",
    "    #n为自变量矩阵行数,即n个样本,p为自变量矩阵列数,即p个参数\n",
    "    \n",
    "    #依次删掉第1至n行样本,利用ridgereg函数得到n个模型的回归参数\n",
    "    coe <- t(apply(xy, 1, mridge, lambda, xtx, xty))\n",
    "    \n",
    "    #计算n个模型的预测值与真实值的距离,再计算这n个距离的平均值(我觉得这里应该是cbind(1, x)*coe) \n",
    "    mean((apply(coe*x, 1, sum) - y)^2)\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "###ridgeregerr函数用于计算训练均方误差  \n",
    "#输入：岭回归超参数lambda，数据矩阵x，因变量y\n",
    "#输出：训练均方误差\n",
    "ridgeregerr=function(lambda,x,y)\n",
    "{\n",
    "    mean((pred(ridgereg(lambda,x,y),x)-y)^2)\n",
    "}   \n",
    "    \n",
    "# cvridgeregerr <- function(lambda, x, y){ \n",
    "#     #mrige函数每次删掉一个样品进行回归,最后得到n个模型的参数 \n",
    "#     np <- dim(x)\n",
    "#     n <- np[1]\n",
    "#     p <- np[2]\n",
    "    \n",
    "#     x <- cbind(1,x)\n",
    "#     xy <- cbind(x,y)\n",
    "    \n",
    "#     xtx=t(x)%*%x + diag(c(0, rep(lambda, p)))\n",
    "#     xty=t(x)%*%y\n",
    "        \n",
    "\n",
    "#     }\n",
    "\n",
    "#     #n为自变量矩阵行数,即n个样本,p为自变量矩阵列数,即p个参数\n",
    "    \n",
    "#     #依次删掉第1至n行样本,利用ridgereg函数得到n个模型的回归参数\n",
    "#     coe <- t(apply(xy, 1, mridge, lambda, xtx, xty))\n",
    "    \n",
    "#     #计算n个模型的预测值与真实值的距离,再计算这n个距离的平均值(我觉得这里应该是cbind(1, x)*coe) 我觉得是一样的，不是点点相乘吗-----某楸乱入\n",
    "#     mean((apply(coe*x, 1, sum) - y)^2)\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "# ###ridgeregerr的参数依次为参数调整比例,自变量,因变量,返回值为bias(只计算一个模型) 我觉得不是bais哦，是整个训练均方误差-----某楸\n",
    "# ridgeregerr <- function(lambda, x, y)\n",
    "# {\n",
    "#   #先利用ridgereg函数回归参数,再输入pred函数求取预测值,最后计算n组样本预测值与真实值距离的平均值\n",
    "#   mean((pred(ridgereg(lambda, x, y), x) - y)^2)\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###############################\n",
    "###在不同的lambda下,比较训练均方误差和测试均方误差，以选取合适的超参数lambda\n",
    "library(ElemStatLearn)\n",
    "x <- as.matrix(prostate[ ,1:8])\n",
    "y <- as.vector(prostate[ ,9])\n",
    "LAM <- seq(0.001, 10, len=50)\n",
    "#计算岭回归50个模型的训练均方误差，将结果从list展开成向量\n",
    "err <- unlist(lapply(LAM, ridgeregerr, x, y))\n",
    "#计算岭回归50个模型的测试均方误差，将结果从list展开成向量\n",
    "pe <- unlist(lapply(LAM, cvridgeregerr, x, y))\n",
    "x <- rep(1:50, 2)\n",
    "plot(pe)\n",
    "#取交叉验证中使测试均方误差最小的lambda\n",
    "lam=LAM[which.min(pe)]\n",
    "\n",
    "\n",
    "###############################################\n",
    "###比较两个回归模型,另一个应该是岭回归\n",
    "library(ElemStatLearn)\n",
    "x <- prostate[ ,1:8]\n",
    "y <- prostate[ ,9]\n",
    "#用本文写的岭回归函数实现岭回归，返回beta估计值\n",
    "ridgereg(lam, x, y)\n",
    "library(mda)\n",
    "#用mda内置函数实现岭回归(输出结果中缺少截距项)\n",
    "ridge1 <- gen.ridge(prostate[ ,1:8], prostate[ ,9, drop <- FALSE], lambda =lam)  \n",
    "ridge1$coe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
