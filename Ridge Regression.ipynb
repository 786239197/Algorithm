{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#一、常用公式\" data-toc-modified-id=\"常用公式-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>常用公式</a></span></li><li><span><a href=\"#二、岭回归原理\" data-toc-modified-id=\"岭回归原理-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>岭回归原理</a></span></li><li><span><a href=\"#三、交叉验证确定$\\lambda$\" data-toc-modified-id=\"交叉验证确定$\\lambda$-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>交叉验证确定$\\lambda$</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本笔记由马景义, 赖楸鸿完成, 如有任何错误欢迎向两位编者反馈."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、 常用公式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.链式法则:若$y(x)$为$x$的向量函数,则\n",
    "$$ \\frac{\\partial f(y(x))}{\\partial x} = \\frac{\\partial y^T(x)}{\\partial x}\\frac{\\partial f(y)}{\\partial y} ,$$\n",
    "其中$\\frac{\\partial y^T(x)}{\\partial x}$为$n\\times n$矩阵."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.$a$为$n\\times1$常数向量,则\n",
    "$$\\frac{\\partial a^Tx}{\\partial x}= a ,$$\n",
    "$$\\frac{\\partial x^Ta}{\\partial x}= a.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.$a$为$n\\times1$常数向量,则\n",
    "$$\\frac{\\partial a^Ty(x)}{\\partial x}=\\frac{\\partial y^T(x)}{\\partial x}a ,$$\n",
    "$$\\frac{\\partial y^T(x)a}{\\partial x}=\\frac{\\partial y^T(x)}{\\partial x}a.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.矩阵$A$和向量$y$与$x$无关,则$$\\frac{\\partial x^TAy}{\\partial x}=Ay ,$$\n",
    "$$\\frac{\\partial y^TAx}{\\partial x}= A^Ty.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.矩阵$A$和向量$y$与$x$无关,则\n",
    "$$\\frac{\\partial x^TA}{\\partial x}=A ,$$\n",
    "$$\\frac{\\partial x^TAx}{\\partial x}=Ax+A^Tx=(A+A^T)x.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、岭回归原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"黑体\">岭回归</font>,又称脊回归、吉洪诺夫正则化(Tikhonov regularization),是对最小二乘的改良,通过牺牲部分偏差换取方差的大幅下降，实现偏差与方差的权衡。\n",
    "通常来说,当响应变量和预测变量的关系接近线性时,最小二乘估计会有较低的偏差和较大的方差,这意味着训练数据的微小改变可能导致最小二乘系数的较大改变.而岭回归引入正则项$\\beta^T\\lambda\\beta,$其中$\\lambda$是一个调节参数,将单独确定.当$\\beta_1,\\cdots,\\beta_p$接近零时,正则项较小,因此具有将估计系数相零的方向压缩的作用.随着$\\lambda$的增加,岭回归拟合结果的光滑度降低,以偏差略微增加为代价,使回归系数缩减,进而方差显著减小,达到权衡效果，测试均方误差减小."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "L(\\beta)=(y-X\\beta)^T(y-X\\beta)+\\beta^T\\lambda\\beta ,$$\n",
    "其中$y$为$n\\times1$向量，$X$为$n\\times p+1$矩阵，$\\beta$为$p+1\\times1$向量，$\\lambda$为$p+1\\times p+1$矩阵,即\n",
    "$$\\lambda= \\left[\\begin{matrix}0 &0&\\cdots&0 \\\\ 0&\\lambda&\\cdots&0\\\\ \\vdots&\\vdots&\\ddots&\\vdots\\\\0&0&\\cdots&\\lambda \\end{matrix}\\right].$$\n",
    "注意到$\\lambda$并未对截距项$\\beta_0$进行惩罚,我们并不想缩减截距项,因为截距项用于测量当$x_{i1},x_{i2},\\cdots,x_{ip}$为零时响应变量的均值.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "岭回归更多细节详见:https://en.wikipedia.org/wiki/Tikhonov_regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对$L(\\beta)$求偏导\n",
    "$$\n",
    "\\begin{split}\n",
    "&\\frac{\\partial L(\\beta)}{\\partial \\beta} \\\\\n",
    "=&\\frac{\\partial (y-X\\beta)^T(y-X\\beta)}{\\partial \\beta}+\\frac{\\partial \\beta^T\\lambda\\beta}{\\partial \\beta} \\\\\n",
    "=&\\frac{\\partial (y-X\\beta)^T}{\\partial \\beta}[(y-X\\beta)+(Y-X\\beta)]+(\\lambda+\\lambda^T)\\beta \\\\\n",
    "=&-2X^T(y-X\\beta)+2\\lambda\\beta\n",
    "\\end{split}\n",
    "$$\n",
    "求解岭回归的问题转化为求解线性方程租$\\frac{\\partial L(\\beta)}{\\partial \\beta}=0$即$(X^TX+\\lambda)\\beta=X^Ty$的问题.\n",
    "我们使用cholesky分解+前/回代法(Forward/Backward Substitution)求解岭回归.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相关内容介绍详见Chol_Solve:此处加超链接,以及cholesky decomposition:https://en.wikipedia.org/wiki/Cholesky_decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###mchol函数将对称方阵分解为一个下三角矩阵乘以该矩阵转置的形式,函数返回值为下三角矩阵\n",
    "#输入：欲分解的矩阵x\n",
    "#输出：cholesky分解所得矩阵L\n",
    "mchol <- function(x)\n",
    "{\n",
    "  #求矩阵x的行列数,m为行数,n为列数\n",
    "  mn <- dim(x)\n",
    "  m <- mn[1]\n",
    "  n <- mn[2]\n",
    "\n",
    "  #检验x是否为方阵\n",
    "  if(m != n) \n",
    "  {\n",
    "    return (\"Wrong dimensions of matrix!\")\n",
    "  }\n",
    "\n",
    "  #检验x是否为对称矩阵\n",
    "  if(sum(t(x) != x) > 0) \n",
    "  {\n",
    "    return (\"Input matrix is not symmetrical!\")\n",
    "  }\n",
    "\n",
    "  #L为与x行列数相等的零矩阵，用于存放分解所得下三角矩阵\n",
    "  L <- matrix(0, m, m)\n",
    "  \n",
    "  #循环每进行一次,求解一列矩阵L的元素\n",
    "  #矩阵x第i列和第i行之前的元素不再使用，相当于矩阵x减少一个维数，故下述将循环所至第i列记为当前矩阵x和矩阵L的第一列\n",
    "  for(i in 1:m)\n",
    "  {\n",
    "    #L的主对角线上第一个元素为x的主对角线上第一个元素开方\n",
    "    L[i,i] <- sqrt(x[i,i])\n",
    "    if(i < m)\n",
    "    {\n",
    "      #求当前矩阵L的第一列除第一个元素外的其他元素\n",
    "      L[(i+1):m,i] <- x[(i+1):m,i]/L[i,i]\n",
    "\n",
    "      #矩阵L第一列（除第一个元素）乘以它的转置得到TLM用于更新矩阵x，效果同TLM%*%TLM\n",
    "      TLV <- L[(i+1):m,i]                               #记录已求出第一列除第一个元素外剩下元素\n",
    "      TLM <- matrix(TLV, m-i, m-i)                      #TLV按列复制成矩阵\n",
    "      TLM <- sweep(TLM, 2, TLV, \"*\")                    #sweep(x， MARGIN， STATS， FUN=”-“， …) 对矩阵进行运算\n",
    "                                                        #MARGIN为1，表示行的方向上进行运算，为2表示列的方向上运算(是指将参数从列的方向移下去算)\n",
    "                                                        #STATS是运算的参数，FUN为运算函数，默认是减法\n",
    "\n",
    "      #减少一个维数的矩阵x更新为原来对应位置上的元素减去TLM，为下一次循环做准备\n",
    "      x[(i+1):m,(i+1):m] <- x[(i+1):m,(i+1):m] - TLM\n",
    "    }\n",
    "  }\n",
    "  #矩阵的返回值为我们要求的下三角矩阵L\n",
    "  L  \n",
    "}\n",
    "\n",
    "######EXAM\n",
    "#y=matrix(rnorm(20),5)\n",
    "#x=t(y)%*%y\n",
    "#mchol(x)\n",
    "######EXAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###mforwardsolve函数求解线性方程租Lx=b，其中L为下三角矩阵\n",
    "#输入：下三角矩阵L，向量b\n",
    "#输出：线性方程组的解x\n",
    "mforwardsolve <- function(L, b)\n",
    "{\n",
    "  #求L的行列数,m为L的行数,n为L的列数\n",
    "  mn <- dim(L)\n",
    "  m <- mn[1]\n",
    "  n <- mn[2]\n",
    "\n",
    "  #判断L是否为方阵\n",
    "  if(m != n) \n",
    "  {\n",
    "    return (\"Wrong dimensions of matrix L!\")\n",
    "  }\n",
    "  \n",
    "  #判断L是否为下三角矩阵\n",
    "  for (i in 1:(m-1))\n",
    "  {\n",
    "    if(sum(L[i,(i+1):m] != 0) > 0)#逐行判断上三角是否全为0元素\n",
    "    {\n",
    "      return (\"Matrix L must be a lower triangular matrix!\")\n",
    "    }\n",
    "  }\n",
    "\n",
    "  #判断L的行数与b的长度是否相等\n",
    "  if(m != length(b))\n",
    "  {\n",
    "    return (\"Wrong dimensions of matrix L or vector b!\")\n",
    "  }\n",
    "  \n",
    "  #0向量记录求解结果\n",
    "  x=rep(0, m)\n",
    "  \n",
    "  #循环每进行一次,求解一个x中的元素，看作矩阵L向量x向量b的维数减一\n",
    "  #故下述将矩阵L的第i列记为当前矩阵L第一列，将向量x向量b的第i个元素记为当前向量第一个元素\n",
    "  for(i in 1:m)\n",
    "  {\n",
    "    #求当前循环中x的第一个元素\n",
    "    x[i] <- b[i] / L[i,i]\n",
    "    #降维后的b向量为原来位置上的元素减去当前矩阵L的第一列的乘积\n",
    "    if(i < m) \n",
    "    {\n",
    "      b[(i+1):m] <- b[(i+1):m] - x[i]*L[(i+1):m,i]\n",
    "    }      \n",
    "  }\n",
    "  #函数返回的x向量即为线性方程组的解\n",
    "  x  \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "###mbacksolve函数求解线性方程租Lx=b，其中L为上三角矩阵\n",
    "#输入：上三角矩阵L，向量b\n",
    "#输出：线性方程组的解x\n",
    "mbacksolve <- function(L, b)\n",
    "{\n",
    "  #求L的行列数,m为L的行数,n为L的列数\n",
    "  mn <-dim(L)\n",
    "  m <- mn[1]\n",
    "  n <- mn[2]\n",
    "\n",
    "  #判断L是否为方阵\n",
    "  if(m != n)\n",
    "  {  \n",
    "    return (\"Wrong dimensions of matrix L!\")\n",
    "  }\n",
    "\n",
    "  #判断L是否为上三角矩阵\n",
    "  for (i in 2:m)\n",
    "  {\n",
    "    if(sum(L[i,1:(i-1)] != 0) > 0)\n",
    "    {\n",
    "      return (\"Matrix L must be a upper triangular matrix!\")\n",
    "    }\n",
    "  }\n",
    "\n",
    "  #判断L的行数与b的列数是否相等\n",
    "  if(m != length(b))\n",
    "  {\n",
    "    return (\"Wrong dimensions of matrix L or vector b!\")\n",
    "  }  \n",
    "  \n",
    "  x <- rep(0, m)\n",
    "  \n",
    "  #循环每进行一次,求解一个x中的元素，看作矩阵L向量x向量b的维数减一\n",
    "  #故下述将矩阵L的第i列记为当前矩阵L最后一列，将向量x向量b的第i个元素记为当前向量最后一个元素\n",
    "  for(i in m:1)\n",
    "  {\n",
    "    #求当前循环中x的最后一个元素\n",
    "    x[i] <- b[i] / L[i,i]\n",
    "    #降维后的向量b为原来位置上的元素减去刚才求出的x元素与当前上三角矩阵L最后一列（除最后一个元素）的乘积\n",
    "    if(i > 1) \n",
    "    {\n",
    "      b[(i-1):1] <- b[(i-1):1] - x[i]*L[(i-1):1,i]\n",
    "    }      \n",
    "  }\n",
    "  #函数返回值x向量即为线性方程组的解\n",
    "  x  \n",
    "}\n",
    "\n",
    "\n",
    "######EXAM\n",
    "#y=matrix(rnorm(20),5)\n",
    "#x=t(y)%*%y\n",
    "#L=mchol(x); b=1:4\n",
    "#mforwardsolve(L,b)\n",
    "#forwardsolve(L,b)\n",
    "######EXAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###ridgereg函数用于实现岭回归参数beta的估计，参数x和y分别为回归方程的自变量和因变量,lambda为L2正则项的调节参数\n",
    "#此函数求解线性方程租(t(x)%*%x+lambada)%*%beta=t(x)%*%y,将t(x)%*%x+lambada进行cholesky分解为R%*%t(R),forwardsolve求解L%*%d=t(x)%*%y,其中d=t(R)%*%beta,backsolve求解t(R)%*%beta=d,即得参数beta的估计值 \n",
    "#输入：自变量x，因变量y，调节参数lambda\n",
    "#输出：回归系数beta的估计值\n",
    "ridgereg <- function(lambda, x, y)\n",
    "{\n",
    "  #y=data[,m]; x=data[,-m]\n",
    "  #n为自变量矩阵行数,即n个样本,p为自变量矩阵列数,即p个参数\n",
    "  np <- dim(x)\n",
    "  n <- np[1]\n",
    "  p <- np[2]\n",
    "\n",
    "  #将自变量矩阵增加一列全1元素,以便于截距项的计算\n",
    "  x <- as.matrix(cbind(rep(1, n),x))\n",
    "\n",
    "  #利用cholesky分解求取回归方程的参数beta的估计值  \n",
    "  V <- t(x)%*%x + diag(c(0, rep(lambda, p)))              #t(x)%*%x+lambda作为线性方程组的系数矩阵V\n",
    "  U <- as.vector(t(x)%*%y)                           \n",
    "  R <- mchol(V)                                           #调用mchol函数将系数矩阵V进行cholesky分解,V=R%*%t(R)\n",
    "  M <- mforwardsolve(R, U)                                #使用前代法求解R%*%M=t(x)%*%y,其中M=t(R)%*%beta\n",
    "  mbacksolve(t(R), M)                                     #使用回代法求解t(R)%*%beta=M,即可得beta的估计值\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "\t<tr><td>4 </td><td> 6</td><td> 8</td></tr>\n",
       "\t<tr><td>6 </td><td> 9</td><td>12</td></tr>\n",
       "\t<tr><td>8 </td><td>12</td><td>16</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{lll}\n",
       "\t 4  &  6 &  8\\\\\n",
       "\t 6  &  9 & 12\\\\\n",
       "\t 8  & 12 & 16\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| 4  |  6 |  8 | \n",
       "| 6  |  9 | 12 | \n",
       "| 8  | 12 | 16 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "     [,1] [,2] [,3]\n",
       "[1,] 4     6    8  \n",
       "[2,] 6     9   12  \n",
       "[3,] 8    12   16  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outer(2:4,2:4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、交叉验证确定$\\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.删一交叉验证(leave-one-out cross-validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"黑体\">删一交叉验证</font>将一个单独$(x_i,y_i)$观测作为验证集,剩下的观测作为训练集,在$n-1$个观测上建模,再用$x_i$预测$y_i$，由于建模过程中没有用到$(x_i,y_i),$故$MSE_i$是测试误差的渐近无偏估计,重复这个步骤$n$次会得到$n$个$MSE_i,$对测试误差的估计即为这$n$估计量的均值."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们用删一交叉验证法选择调节参数$\\lambda,$选择一系列$\\lambda$的值,计算每个$\\lambda$对应模型的测试误差估计值,选择其中测试误差估计值最小的$\\lambda.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "删一交叉验证详见：https://en.wikipedia.org/wiki/Cross-validation_(statistics)#Leave-one-out_cross-validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>0.439533558041209</li>\n",
       "\t<li>0.562800409300066</li>\n",
       "\t<li>0.561761682327131</li>\n",
       "\t<li>-0.0198710012557136</li>\n",
       "\t<li>0.0989502953509072</li>\n",
       "\t<li>0.645197525205287</li>\n",
       "\t<li>-0.0777548597849345</li>\n",
       "\t<li>0.0344279766734407</li>\n",
       "\t<li>0.00465563598611401</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 0.439533558041209\n",
       "\\item 0.562800409300066\n",
       "\\item 0.561761682327131\n",
       "\\item -0.0198710012557136\n",
       "\\item 0.0989502953509072\n",
       "\\item 0.645197525205287\n",
       "\\item -0.0777548597849345\n",
       "\\item 0.0344279766734407\n",
       "\\item 0.00465563598611401\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 0.439533558041209\n",
       "2. 0.562800409300066\n",
       "3. 0.561761682327131\n",
       "4. -0.0198710012557136\n",
       "5. 0.0989502953509072\n",
       "6. 0.645197525205287\n",
       "7. -0.0777548597849345\n",
       "8. 0.0344279766734407\n",
       "9. 0.00465563598611401\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1]  0.439533558  0.562800409  0.561761682 -0.019871001  0.098950295\n",
       "[6]  0.645197525 -0.077754860  0.034427977  0.004655636"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>lpsa</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td> 0.562800409</td></tr>\n",
       "\t<tr><td> 0.561761682</td></tr>\n",
       "\t<tr><td>-0.019871001</td></tr>\n",
       "\t<tr><td> 0.098950295</td></tr>\n",
       "\t<tr><td> 0.645197525</td></tr>\n",
       "\t<tr><td>-0.077754860</td></tr>\n",
       "\t<tr><td> 0.034427977</td></tr>\n",
       "\t<tr><td> 0.004655636</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{l}\n",
       " lpsa\\\\\n",
       "\\hline\n",
       "\t  0.562800409\\\\\n",
       "\t  0.561761682\\\\\n",
       "\t -0.019871001\\\\\n",
       "\t  0.098950295\\\\\n",
       "\t  0.645197525\\\\\n",
       "\t -0.077754860\\\\\n",
       "\t  0.034427977\\\\\n",
       "\t  0.004655636\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "lpsa | \n",
       "|---|---|---|---|---|---|---|---|\n",
       "|  0.562800409 | \n",
       "|  0.561761682 | \n",
       "| -0.019871001 | \n",
       "|  0.098950295 | \n",
       "|  0.645197525 | \n",
       "| -0.077754860 | \n",
       "|  0.034427977 | \n",
       "|  0.004655636 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "     lpsa        \n",
       "[1,]  0.562800409\n",
       "[2,]  0.561761682\n",
       "[3,] -0.019871001\n",
       "[4,]  0.098950295\n",
       "[5,]  0.645197525\n",
       "[6,] -0.077754860\n",
       "[7,]  0.034427977\n",
       "[8,]  0.004655636"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nAYQEAYQEAYQEAYQEAYQEAYQEAf4DFBAqD3dY0oMAAAAASUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###pred函数的参数b为参数向量,x为自变量,返回值为因变量的预测值\n",
    "#输入：回归系数b向量,数据nx\n",
    "#输出：因变量y的预测值\n",
    "pred <- function(b, nx)\n",
    "{\n",
    "  #nx=prostate[1:2,1:8]\n",
    "  b <- as.vector(b)\n",
    "  p <- length(b) - 1\n",
    "  \n",
    "  #将数据矩阵nx重新排列，每一行为一个样品，重排矩阵的原因是下面例子中调用的数据原结构为dataframe\n",
    "  nx <- as.matrix(nx, ncol <- p)\n",
    "  n <- dim(nx)[1]\n",
    "  \n",
    "  #计算预测值\n",
    "  apply(t(nx)*b[2:(p+1)], 2, sum) + b[1]  \n",
    "}\n",
    "\n",
    "###mridge函数用于实现删去一个样品的岭回归\n",
    "mridge=function(i,lambda,x,y) \n",
    "{\n",
    "    ridgereg(lambda,x[-i,],y[-i])\n",
    "}\n",
    "\n",
    "###cvridgeregerr函数用交叉验证实现岭回归，参数依次为调节参数lambda,自变量x(数据矩阵),因变量y,返回值为测试均方误差   \n",
    "#输入：超参数lambda,自变量x(数据矩阵),因变量y\n",
    "#输出：删一交叉验证岭回归测试均方误差\n",
    "cvridgeregerr<-function(lambda,x,y){  \n",
    "    #lambda=1\n",
    "    np<-dim(x)\n",
    "    n<-np[1]\n",
    "    p<-np[2]\n",
    "    #矩阵中的元素作为第一个参数输入mridge，表示去掉的数据编号，结果第i行为删去第i个样本的岭回归系数估计值\n",
    "    coe<-t(apply(as.matrix(1:n,ncol=1),1,mridge,lambda,x,y))\n",
    "    #coe第i行和数据矩阵第i个样本做点对点相乘，对行求和，计算测试均方误差\n",
    "    mean((apply(coe*cbind(1,x),1,sum)-y)^2)\n",
    "}\n",
    "\n",
    "###ridgeregerr函数用于计算训练均方误差  \n",
    "#输入：岭回归超参数lambda，数据矩阵x，因变量y\n",
    "#输出：训练均方误差\n",
    "ridgeregerr=function(lambda,x,y)\n",
    "{\n",
    "    mean((pred(ridgereg(lambda,x,y),x)-y)^2)\n",
    "}   \n",
    "    \n",
    "###############################\n",
    "###在不同的lambda下,比较训练均方误差和测试均方误差，以选取合适的调节参数lambda\n",
    "library(ElemStatLearn)\n",
    "x <- as.matrix(prostate[ ,1:8])\n",
    "y <- as.vector(prostate[ ,9])\n",
    "LAM <- seq(0.001, 10, len=50)\n",
    "#计算岭回归50个模型的训练均方误差，将结果从list展开成向量\n",
    "err <- unlist(lapply(LAM, ridgeregerr, x, y))\n",
    "#计算岭回归50个模型的测试均方误差，将结果从list展开成向量\n",
    "pe <- unlist(lapply(LAM, cvridgeregerr, x, y))\n",
    "x <- rep(1:50, 2)\n",
    "plot(pe)\n",
    "#取交叉验证中使测试均方误差最小的lambda\n",
    "lam=LAM[which.min(pe)]\n",
    "\n",
    "\n",
    "###############################################\n",
    "###比较R内置函数与本文的岭回归函数\n",
    "library(ElemStatLearn)\n",
    "x <- prostate[ ,1:8]\n",
    "y <- prostate[ ,9]\n",
    "#用本文写的岭回归函数实现岭回归，返回beta估计值\n",
    "ridgereg(lam, x, y)\n",
    "library(mda)\n",
    "#用mda内置函数实现岭回归(输出结果中缺少截距项)\n",
    "ridge1 <- gen.ridge(prostate[ ,1:8], prostate[ ,9, drop <- FALSE], lambda =lam)  \n",
    "ridge1$coe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.K折交叉验证(k-fold cross-validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"黑体\">K折交叉验证</font>将数据集随机地分成$k$个大小基本一致的组,或者说折.其中一折作为验证集,剩下$k-1$折作为训练集,在训练集上建模,在验证集上计算测试均方误差的估计,重复这个步骤$k$次,得到$k$个测试误差的估计,对测试误差的估计即为$k$个估计量的均值."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K折交叉验证详见:https://en.wikipedia.org/wiki/Cross-validation_(statistics)#k-fold_cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "1.18367346938776"
      ],
      "text/latex": [
       "1.18367346938776"
      ],
      "text/markdown": [
       "1.18367346938776"
      ],
      "text/plain": [
       "[1] 1.183673"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>0.376187864929886</li>\n",
       "\t<li>0.563524400352627</li>\n",
       "\t<li>0.577063360389855</li>\n",
       "\t<li>-0.0202229047083941</li>\n",
       "\t<li>0.0983672737941701</li>\n",
       "\t<li>0.673266376910463</li>\n",
       "\t<li>-0.0847834674506864</li>\n",
       "\t<li>0.0378516961588805</li>\n",
       "\t<li>0.00461118298222048</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 0.376187864929886\n",
       "\\item 0.563524400352627\n",
       "\\item 0.577063360389855\n",
       "\\item -0.0202229047083941\n",
       "\\item 0.0983672737941701\n",
       "\\item 0.673266376910463\n",
       "\\item -0.0847834674506864\n",
       "\\item 0.0378516961588805\n",
       "\\item 0.00461118298222048\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 0.376187864929886\n",
       "2. 0.563524400352627\n",
       "3. 0.577063360389855\n",
       "4. -0.0202229047083941\n",
       "5. 0.0983672737941701\n",
       "6. 0.673266376910463\n",
       "7. -0.0847834674506864\n",
       "8. 0.0378516961588805\n",
       "9. 0.00461118298222048\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1]  0.376187865  0.563524400  0.577063360 -0.020222905  0.098367274\n",
       "[6]  0.673266377 -0.084783467  0.037851696  0.004611183"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>lpsa</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td> 0.563524400</td></tr>\n",
       "\t<tr><td> 0.577063360</td></tr>\n",
       "\t<tr><td>-0.020222905</td></tr>\n",
       "\t<tr><td> 0.098367274</td></tr>\n",
       "\t<tr><td> 0.673266377</td></tr>\n",
       "\t<tr><td>-0.084783467</td></tr>\n",
       "\t<tr><td> 0.037851696</td></tr>\n",
       "\t<tr><td> 0.004611183</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{l}\n",
       " lpsa\\\\\n",
       "\\hline\n",
       "\t  0.563524400\\\\\n",
       "\t  0.577063360\\\\\n",
       "\t -0.020222905\\\\\n",
       "\t  0.098367274\\\\\n",
       "\t  0.673266377\\\\\n",
       "\t -0.084783467\\\\\n",
       "\t  0.037851696\\\\\n",
       "\t  0.004611183\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "lpsa | \n",
       "|---|---|---|---|---|---|---|---|\n",
       "|  0.563524400 | \n",
       "|  0.577063360 | \n",
       "| -0.020222905 | \n",
       "|  0.098367274 | \n",
       "|  0.673266377 | \n",
       "| -0.084783467 | \n",
       "|  0.037851696 | \n",
       "|  0.004611183 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "     lpsa        \n",
       "[1,]  0.563524400\n",
       "[2,]  0.577063360\n",
       "[3,] -0.020222905\n",
       "[4,]  0.098367274\n",
       "[5,]  0.673266377\n",
       "[6,] -0.084783467\n",
       "[7,]  0.037851696\n",
       "[8,]  0.004611183"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nIEBIECAkCBASBAgJAv4D+iA4iEE2m5EAAAAASUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "k_ridge=function(i, lambda, x, y, index) \n",
    "{\n",
    "    ridgereg(lambda,x[!(index==i),],y[!(index==i)])\n",
    "}\n",
    "\n",
    "###k_cvridgeregerr函数用K折交叉验证实现岭回归，参数依次为调节参数lambda,自变量x(数据矩阵),因变量y,返回值为测试均方误差的估计   \n",
    "#输入：调节参数lambda,自变量x(数据矩阵),因变量y，折数k(一般选取5或10)\n",
    "#输出：k折交叉验证岭回归测试均方误差\n",
    "k_cvridgeregerr<-function(lambda, x, y, k){  \n",
    "    np <- dim(x)\n",
    "    n <- np[1]\n",
    "    index <- sample(rep(1:k,ceiling(n/k)), n)\n",
    "\n",
    "    #矩阵中的元素作为当前删去折数作为参数传入k_ridge，结果第i行为删去第i折的岭回归系数估计值\n",
    "    coe <- t(apply(as.matrix(1:k,ncol=1),1,k_ridge,lambda, x, y, index))\n",
    "    #coe对应系数和数据矩阵做点对点相乘，对行求和，计算测试均方误差的估计\n",
    "    mean((apply(coe[index,]*cbind(1,x),1,sum)-y)^2)\n",
    "}\n",
    "  \n",
    "###############################\n",
    "###在不同的lambda下,计算测试均方误差的估计，以选取合适的调节参数lambda\n",
    "library(ElemStatLearn)\n",
    "x <- as.matrix(prostate[,1:8])\n",
    "y <- as.vector(prostate[,9])\n",
    "LAM <- seq(1, 2, len=50)\n",
    "#计算岭回归50个模型的测试均方误差，将结果从list展开成向量\n",
    "pe <- unlist(lapply(LAM, k_cvridgeregerr, x, y, k=10))\n",
    "plot(pe)\n",
    "\n",
    "# #取交叉验证中使测试均方误差最小的lambda\n",
    "lam=LAM[which.min(pe)]\n",
    "lam\n",
    "\n",
    "\n",
    "###############################################\n",
    "###比较R内置函数与本文的岭回归函数\n",
    "library(ElemStatLearn)\n",
    "x <- prostate[ ,1:8]\n",
    "y <- prostate[ ,9]\n",
    "#用本文写的岭回归函数实现岭回归，返回beta估计值\n",
    "ridgereg(lam, x, y)\n",
    "library(mda)\n",
    "#用mda内置函数实现岭回归(输出结果中缺少截距项)\n",
    "ridge1 <- gen.ridge(prostate[ ,1:8], prostate[ ,9, drop <- FALSE], lambda =lam)  \n",
    "ridge1$coe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
